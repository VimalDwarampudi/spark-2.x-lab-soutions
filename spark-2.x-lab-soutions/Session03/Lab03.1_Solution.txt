*** Section: Process a large file ***

* In the Spark Shell, load the *100M.data* file
scala> val f = sc.textFile("spark-labs/data/twinkle/100M.data")
* Count the number of lines that have the word "diamond"
  scala> val filtered = f.filter(line => line.contains("diamond"))
* Check how many 'tasks' are used in the above calculation
  Shows 4 - depends on number of cores/threads available, and size of data to be processed.

* How do you get the total line count?
 scala>f.count  // Simple
 
* Count the number of lines that do NOT have the word 'diamond'**
val filtered = f.filter(line => !line.contains("diamond"))  // Note the ! before line.contains

*** Section: Loading multiple files ***

* To load all our twinkle .data files, then filter for lines containing diamond. (Make sure to unzip the large files first)
val f = sc.textFile("spark-labs/data/twinkle/*.data")
val filtered = f.filter(line => line.contains("diamond"))

*** Section: Writing data ***

* [Optional]: Try writing the filtered RDD as a single file
filtered.coalesce(1).saveAsTextFile("spark-labs/data/twinkle/out2")	

